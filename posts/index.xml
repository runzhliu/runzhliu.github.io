<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on runzhliu</title>
    <link>https://runzhliu.github.io/posts/</link>
    <description>Recent content in Posts on runzhliu</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sat, 20 Jul 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://runzhliu.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Flink Dockerfile 走读</title>
      <link>https://runzhliu.github.io/posts/flink-dockerfile-%E8%B5%B0%E8%AF%BB/</link>
      <pubDate>Sat, 20 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://runzhliu.github.io/posts/flink-dockerfile-%E8%B5%B0%E8%AF%BB/</guid>
      <description>1 Overview 关于 Flink 的 Docker 相关的配置，可以参考源码这个目录。
/path/to/flink/flink-container/docker ├── Dockerfile // Dockerfile ├── README.md // 具体的说明，如何创建 Flink 的镜像文件 ├── build.sh // ├── docker-compose.yml // └── docker-entrypoint.sh // Dockerfile 中运行的脚本 2 Dockerfile # 省略了 License，特此声明 FROM openjdk:8-jre-alpine # 安装需要的软件 # snappy 是一个压缩库 # libc6-compat 是 ANSI C 的函数库 RUN apk add --no-cache bash snappy libc6-compat # Flink 容器里的环境变量 # Flink 软件的安装目录在 /opt ENV FLINK_INSTALL_PATH=/opt # Flikn 的解压目录在 /opt/flink ENV FLINK_HOME $FLINK_INSTALL_PATH/flink # Flink 的依赖包目录在 /opt/flink/lib ENV FLINK_LIB_DIR $FLINK_HOME/lib # Flink 的插件目录在 /opt/flink/plugins ENV FLINK_PLUGINS_DIR $FLINK_HOME/plugins # 这个不知道是什么目录 ENV FLINK_OPT_DIR $FLINK_HOME/opt # 这是用户代码的 Jar 包目录，/opt/flink/artifacts ENV FLINK_JOB_ARTIFACTS_DIR $FLINK_INSTALL_PATH/artifacts # 更新一下 PATH，把 Flink 的二进制文件的目录加上 /opt/flink/bin ENV PATH $PATH:$FLINK_HOME/bin # 这些 ARG 可以在构建镜像的时候输入参数，默认值都是 NOT_SET，如果设置了就会去找对应的目录，并且打入镜像里 # Flink 的发行版路径，可以在本地指定任何下载或者自行打包的 Flink 发行版包 ARG flink_dist=NOT_SET # 用户写的业务代码路径 ARG job_artifacts=NOT_SET # Python 的版本，填2或者3 ARG python_version=NOT_SET # Hadoop Jar 包的依赖路径 ARG hadoop_jar=NOT_SET* # 安装 Python，根据前面填的 python_version 这个环境变量，不填就不装 RUN \ if [ &amp;#34;$python_version&amp;#34; = &amp;#34;2&amp;#34; ]; then \ apk add --no-cache python; \ elif [ &amp;#34;$python_version&amp;#34; = &amp;#34;3&amp;#34; ]; then \ apk add --no-cache python3 &amp;amp;&amp;amp; ln -s /usr/bin/python3 /usr/bin/python; \ fi # 把 Flink 发行版和 Hadoop jar（不一定有 Hadoop）放在 /opt/flink 目录 ADD $flink_dist $hadoop_jar $FLINK_INSTALL_PATH/ # 用户代码放在 /opt/artifacts ADD $job_artifacts/* $FLINK_JOB_ARTIFACTS_DIR/ RUN set -x &amp;amp;&amp;amp; \ ln -s $FLINK_INSTALL_PATH/flink-[0-9]* $FLINK_HOME &amp;amp;&amp;amp; \ for jar in $FLINK_JOB_ARTIFACTS_DIR/*.</description>
    </item>
    
    <item>
      <title>Flink 集群-任务容器化</title>
      <link>https://runzhliu.github.io/posts/flink-%E9%9B%86%E7%BE%A4-%E4%BB%BB%E5%8A%A1%E5%AE%B9%E5%99%A8%E5%8C%96/</link>
      <pubDate>Sat, 20 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://runzhliu.github.io/posts/flink-%E9%9B%86%E7%BE%A4-%E4%BB%BB%E5%8A%A1%E5%AE%B9%E5%99%A8%E5%8C%96/</guid>
      <description>1 Overview /path/to/flink/flink-container/docker ├── Dockerfile ├── README.md ├── build.sh ├── docker-compose.yml ├── docker-entrypoint.sh └── test-job-cluster.sh 0 directories, 6 files Flink Dockerfile 走读已经介绍了 Flink 的镜像应该如何构建了，接下来，本文解释一下如何利用 Docker 来部署 Flink。
2 Docker Compose 以下是 docker-compose.yml 的内容。
# 省略 License # Docker compose file for a Flink job cluster deployment. # 注意下面这些参数的设置 # Parameters: # * FLINK_DOCKER_IMAGE_NAME - Image name to use for the deployment (default: flink-job:latest) # * FLINK_JOB - Name of the Flink job to execute (default: none) # * DEFAULT_PARALLELISM - Default parallelism with which to start the job (default: 1) # * FLINK_JOB_ARGUMENTS - Additional arguments which will be passed to the job cluster (default: none) # * SAVEPOINT_OPTIONS - Savepoint options to start the cluster with (default: none) version: &amp;#34;2.</description>
    </item>
    
    <item>
      <title>Flink Session Cluster on K8S</title>
      <link>https://runzhliu.github.io/posts/flink-session-cluster-on-k8s/</link>
      <pubDate>Fri, 19 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://runzhliu.github.io/posts/flink-session-cluster-on-k8s/</guid>
      <description>1 Overview 本文是根据官方文档略加编辑整理出来的。
2 Setup Kubernetes Mac 环境推荐 Docker for Mac，一键部署。
实验过程中，应该注意你的 K8S 的版本等信息。
2 Flink session cluster on Kubernetes Flink session cluster 是作为 K8S 的 Deployment，Flink 的作业会被提交到 session cluster。至于什么是 Deployment，不清楚的同学可以看Deployment。Flink session cluster 会包含以下组件:
 JobManager 以 Deployment 的方式运行在 K8S 集群 TaskManagers 也是以 Deployment 的方式运行在 K8S 集群 JobManager 的 REST 和 UI 端口通过 Service 部署在 K8S 集群  2.1 Deploy Flink session cluster on Kubernetes 请按照官网的 Appendix将几个文件拷贝到本地。
然后就是部署，按照以下命令。
kubectl create -f jobmanager-service.</description>
    </item>
    
    <item>
      <title>Mac CPU 相关</title>
      <link>https://runzhliu.github.io/posts/mac-cpu-%E7%9B%B8%E5%85%B3/</link>
      <pubDate>Thu, 18 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://runzhliu.github.io/posts/mac-cpu-%E7%9B%B8%E5%85%B3/</guid>
      <description>1 Monitor  https://support.apple.com/zh-cn/HT201464
 这个是 Mac 自带的监视器，可以监视 包括 CPU，内存在内的多种资源，使用简单，可以针对某个进程进行 Kill。
2 命令模式 2.1 system_profiler SPHardwareDataType 2.2 sysctl sysctl 是可以提取内核状态的命令，具体用法可以 man sysctl 获取全面的手册，以下是从手册中获取的一些与 CPU 有关的指标。
   Name Type Changeable     hw.activecpu integer no   hw.cpu64bit_capable integer no   hw.cpufamily integer no   hw.cpufrequency integer no   hw.cpufrequency_max integer no   hw.cpufrequency_min integer no   hw.cpusubtype integer no   hw.</description>
    </item>
    
    <item>
      <title>Spark RDD依赖的深度优先搜索</title>
      <link>https://runzhliu.github.io/posts/spark-rdd%E4%BE%9D%E8%B5%96%E7%9A%84%E6%B7%B1%E5%BA%A6%E4%BC%98%E5%85%88%E6%90%9C%E7%B4%A2/</link>
      <pubDate>Thu, 18 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://runzhliu.github.io/posts/spark-rdd%E4%BE%9D%E8%B5%96%E7%9A%84%E6%B7%B1%E5%BA%A6%E4%BC%98%E5%85%88%E6%90%9C%E7%B4%A2/</guid>
      <description>1 Overview 最近在刷刷算法题，看到经典的树搜索的算法，正巧之前记得 Spark RDD 中有一处利用 DFS 来判断 RDD 依赖关系的代码，因此专门拿出来分析一下。
2 Code /** * Return the ancestors of the given RDD that are related to it only through a sequence of * narrow dependencies. This traverses the given RDD&amp;#39;s dependency tree using DFS, but maintains * no ordering on the RDDs returned. */ private[spark] def getNarrowAncestors: Seq[RDD[_]] = { val ancestors = new mutable.HashSet[RDD[_]] def visit(rdd: RDD[_]): Unit = { val narrowDependencies = rdd.</description>
    </item>
    
  </channel>
</rss>